{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6408d75-e80a-49dd-ab25-7462584e854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics, Dataset\n",
    "from typing import NamedTuple\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca5698-7fc6-4121-aea8-cf9e136b812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'qwiklabs-gcp-03-6e0d35a97dd4'\n",
    "pipeline_root_path = 'gs://pipeline-tester3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4a2a6-990f-4dcb-a0f2-cedad0131e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INGEST THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb78d84c-2a34-436d-9a4b-5d1cf956fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['tensorflow==2.11.0', 'tensorflow-datasets', 'numpy==1.21.6']\n",
    ")\n",
    "def ingest_data() -> str:\n",
    "    import tensorflow_datasets as tfds\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "\n",
    "    #bucket = 'gs://tfds-dir3'\n",
    "    #bucket = 'gs://pipeline-tester3/keras_model'\n",
    "    bucket = 'gs://workbench-ron-tensor'\n",
    "\n",
    "    (ds_train, ds_test), ds_info = tfds.load(\n",
    "        'cifar10',\n",
    "        split=['train', 'test'],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "\n",
    "    def normalize_img(image, label):\n",
    "        \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "        return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "    ds_train = ds_train.map(\n",
    "        normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    ds_test = ds_test.map(\n",
    "        normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "   # need the \"self\" parameter as their is an implicit argument in the custom_shard_func\n",
    "   #  that gives an error saying one arg expected but two were given\n",
    "    def custom_shard_func(self, element):\n",
    "        return np.int64(0)\n",
    "\n",
    "    ds_train.save(path=bucket + \"/ds_train\", shard_func=custom_shard_func)\n",
    "    ds_test.save(path=bucket + \"/ds_test\", shard_func=custom_shard_func)\n",
    "\n",
    "    return bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e525ff9-98f1-42d8-b21e-1de7e960a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2dc65-40af-4304-87d0-a789dc816b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['tensorflow==2.11.0', 'keras'],\n",
    "    base_image='gcr.io/deeplearning-platform-release/tf-gpu.2-11',\n",
    ")\n",
    "def create_model(text: str) -> str:\n",
    "    import tensorflow as tf\n",
    "    from keras import models, layers, datasets, applications\n",
    "\n",
    "    #bucket = 'gs://tfds-dir3'\n",
    "    #bucket = 'gs://pipeline-tester3/keras_model'\n",
    "    bucket = 'gs://workbench-ron-tensor'\n",
    "\n",
    "    # check for GPU:\n",
    "    print('\\n\\n GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print('\\n\\n')\n",
    "\n",
    "    #Multi GPU strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        model.add(layers.Dense(10))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print('\\n\\n' + str(model.summary()) + '\\n\\n')\n",
    "    model.save(bucket + \"/untrained-model\")\n",
    "    return \"model saved:\" + bucket\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17596be6-7735-4c6d-a2ba-cc4365a285fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b04618-33d5-495d-a60b-091ec29d1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['tensorflow==2.11.0', 'tensorflow-datasets'],\n",
    "    output_component_file=\"train_model.yaml\",\n",
    "    base_image='gcr.io/deeplearning-platform-release/tf-gpu.2-11',\n",
    ")\n",
    "def train_model(text: str) -> str:\n",
    "    import tensorflow_datasets as tfds\n",
    "    import tensorflow as tf\n",
    "\n",
    "\n",
    "    # check for GPU:\n",
    "    print('\\n\\n GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print('\\n\\n')\n",
    "\n",
    "    #Storage buckets\n",
    "    # bucket = 'gs://tfds-dir3'\n",
    "    # bucket = 'gs://pipeline-tester3/keras_model'\n",
    "    bucket = 'gs://workbench-ron-tensor'\n",
    "\n",
    "    #Multi GPU strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "   \n",
    "\n",
    "    # load data from gcs bucket\n",
    "    ds_train = tf.data.Dataset.load(bucket + \"/ds_train\")\n",
    "    ds_test = tf.data.Dataset.load(bucket + \"/ds_test\")\n",
    "\n",
    "    print(\"\\n\\n finish loading training data\")\n",
    "    print(\"\\n\\n finish loading test data \\n\\n\")\n",
    "\n",
    "    # load model from gcs\n",
    "    model = tf.keras.models.load_model(bucket + '/untrained-model')\n",
    "\n",
    "    # Create and Train the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=30,\n",
    "    validation_data=ds_test,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n history\" + str(history))\n",
    "\n",
    "    #Save the model\n",
    "    model.save(bucket + \"/keras_ model\")\n",
    "\n",
    "    return \"model trained\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6d29f-9270-48c6-aa91-7e1320df09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112a9a8-9679-4655-974d-36d5cfaeb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='simple-pipeline',\n",
    "    description='testing pipeline',\n",
    "    pipeline_root=pipeline_root_path\n",
    ")\n",
    "def workbench_cnn_pipeline(    \n",
    "    bucket: str = 'gs://workbench-ron-tensor',\n",
    "    project: str = 'tensor-1-1',\n",
    "):\n",
    "    ingestion_task = ingest_data()\n",
    "    create_model_task = create_model(text=ingestion_task.output).set_accelerator_type('NVIDIA_TESLA_V100').set_cpu_limit('4').set_memory_limit('16G').set_accelerator_limit(4)\n",
    "    train_model_task_4_V100_GPUs = train_model(text=create_model_task.output)\\\n",
    "        .set_accelerator_type('NVIDIA_TESLA_V100')\\\n",
    "        .set_cpu_limit('4')\\\n",
    "        .set_memory_limit('16G')\\\n",
    "        .set_accelerator_limit(4)\\\n",
    "        .set_display_name('4 x V100 GPUS ')\n",
    "    train_model_task_2_V100_GPUs = train_model(text=create_model_task.output)\\\n",
    "        .set_accelerator_type('NVIDIA_TESLA_V100')\\\n",
    "        .set_cpu_limit('4')\\\n",
    "        .set_memory_limit('16G')\\\n",
    "        .set_accelerator_limit(2)\\\n",
    "        .set_display_name('2 x V100 GPUS ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44302d-6d65-457a-9fb8-fab041170e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE THE PIPELINE TO CREATE THE PIPELINE JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12caac78-661b-496f-bef2-78c7143a4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    compiler.Compiler().compile(\n",
    "        pipeline_func=workbench_cnn_pipeline,\n",
    "        package_path='workbench_cnn_pipeline.json'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4cff8e-7434-4ee8-b48e-f7c32dfff745",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4352e-1c0d-4d2b-b67b-0d9eae980f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "job = aiplatform.PipelineJob(display_name = 'workbench_cnn_pipeline',\n",
    "                             template_path = 'simple-model-pipeline.json',\n",
    "                             enable_caching=True,\n",
    "                             )\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09197e1c-3afc-4526-afec-e25ccd1ab841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MODEL TO MODEL REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b32a932-dfb0-4388-b237-2c0d8ab0dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storage buckets\n",
    "# bucket = 'gs://tfds-dir3'\n",
    "# bucket = 'gs://pipeline-tester3'\n",
    "bucket = 'gs://workbench-ron-tensor'\n",
    "model_uri = bucket + \"/keras_ model\"\n",
    "    \n",
    "my_model = aiplatform.Model.upload(display_name='keras-model', \n",
    "                                   artifact_uri=model_uri, \n",
    "                                   serving_container_image_uri= 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82cc41-dd52-4426-8ca6-3891fc0f3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPLOY MODEL TO ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7d804-9b89-42d6-980f-09a73ac3b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model = aiplatform.Model(\"projects/{PROJECT_NUMBER}/locations/us-central1/models/{MODEL_ID}\") \n",
    "my_model = aiplatform.Model('projects/10566138111/locations/us-central1/models/9169691680163495936@1')\n",
    "\n",
    "\n",
    "endpoint = my_model.deploy(\n",
    "     deployed_model_display_name='keras-endpoint',\n",
    "     traffic_split={\"0\": 100},\n",
    "     machine_type=\"n1-standard-4\",\n",
    "     accelerator_count=0,\n",
    "     min_replica_count=1,\n",
    "     max_replica_count=1,\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cf548-c392-49b5-81ed-13c644bcd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET PREDICTIONS FROM DEPLOYED LOCAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a2009be8-58cf-48b5-bfdb-3cda7379ca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step\n",
      "The local model predicts this image most likely belongs to automobile with a 23.20 percent confidence.\n",
      "gs://pipeline-tester3\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import base64\n",
    "from typing import List, Dict\n",
    "import argparse\n",
    "import io\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "path_image = \"/home/jupyter/tensor project/tensorflow/workbench_demo/test_images/car.jpg\"\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    path_image, target_size=(32, 32))\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "model = tf.keras.models.load_model(bucket + \"/keras_ model\")\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(\n",
    "    \"The local model predicts this image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n",
    "\n",
    "print(bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3f4a1-c33e-404c-94d1-617b46af498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET PREDICTIONS FROM MODEL DEPLOYED AT ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4fefcc5d-e87a-4edf-9974-238545b43ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "The Vertex AI Endpoint predicts this image most likely belongs to dog with a 23.20 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import base64\n",
    "from typing import List, Dict\n",
    "import argparse\n",
    "import io\n",
    "import tensorflow as tf\n",
    "\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    endpoint_name='projects/10566138111/locations/us-central1/endpoints/4942326757085675520')\n",
    "\n",
    "path_image = \"/home/jupyter/tensor project/tensorflow/workbench_demo/test_images/car.jpg\"\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "\n",
    "image_data = [np.asarray(Image.open(path_image))]\n",
    "test_image = [(image_data[0] / 255.0).astype(np.float32).tolist()]\n",
    "\n",
    "endpoint_prediction = endpoint.predict(instances=test_image)\n",
    "endpoint_score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\"\\n\" + \"\\n\")\n",
    "print(\n",
    "    \"The Vertex AI Endpoint predicts this image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(endpoint_score)], 100 * np.max(endpoint_score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "db70d553-7985-472c-ae48-8d7c0bc0ed98",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_15704/3989289774.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Extract labels from image name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/ipykernel_15704/3989289774.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Extract labels from image name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import base64\n",
    "from typing import List, Dict\n",
    "import argparse\n",
    "import io\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    endpoint_name='projects/10566138111/locations/us-central1/endpoints/4942326757085675520')\n",
    "\n",
    "\n",
    "# Load image data\n",
    "IMAGE_DIRECTORY = \"/home/jupyter/tensor project/tensorflow/workbench_demo/test_images\"\n",
    "\n",
    "image_files = [file for file in os.listdir(IMAGE_DIRECTORY) if file.endswith(\".jpg\")]\n",
    "\n",
    "# Decode JPEG images into numpy arrays\n",
    "image_data = [\n",
    "    np.asarray(Image.open(os.path.join(IMAGE_DIRECTORY, file))) for file in image_files\n",
    "]\n",
    "\n",
    "# Scale and convert to expected format\n",
    "x_test = [(image / 255.0).astype(np.float32).tolist() for image in image_data]\n",
    "\n",
    "# Extract labels from image name\n",
    "y_test = [int(file.split(\"_\")[1]) for file in image_files]\n",
    "\n",
    "\n",
    "predictions = endpoint.predict(instances=x_test)\n",
    "y_predicted = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "correct = sum(y_predicted == np.array(y_test))\n",
    "accuracy = len(y_predicted)\n",
    "print(\n",
    "    f\"Correct predictions = {correct}, Total predictions = {accuracy}, Accuracy = {correct/accuracy}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983579c5-6799-47ed-9871-7bc14b55f3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab195115-198a-4465-b3ba-dcb36e015d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
