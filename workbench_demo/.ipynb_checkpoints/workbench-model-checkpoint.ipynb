{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a6408d75-e80a-49dd-ab25-7462584e854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics, Dataset\n",
    "from typing import NamedTuple\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "61ca5698-7fc6-4121-aea8-cf9e136b812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'qwiklabs-gcp-03-6e0d35a97dd4'\n",
    "pipeline_root_path = 'gs://pipeline-tester3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5ab4a2a6-990f-4dcb-a0f2-cedad0131e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INGEST THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "eb78d84c-2a34-436d-9a4b-5d1cf956fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['tensorflow==2.11.0', 'tensorflow-datasets', 'numpy==1.21.6']\n",
    ")\n",
    "def ingest_data() -> str:\n",
    "    import tensorflow_datasets as tfds\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "\n",
    "    #bucket = 'gs://tfds-dir3'\n",
    "    #bucket = 'gs://pipeline-tester3/keras_model'\n",
    "    bucket = 'gs://workbench-ron-tensor'\n",
    "\n",
    "    (ds_train, ds_test), ds_info = tfds.load(\n",
    "        'cifar10',\n",
    "        split=['train', 'test'],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "\n",
    "    def normalize_img(image, label):\n",
    "        \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "        return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "    ds_train = ds_train.map(\n",
    "        normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    ds_test = ds_test.map(\n",
    "        normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "   # need the \"self\" parameter as their is an implicit argument in the custom_shard_func\n",
    "   #  that gives an error saying one arg expected but two were given\n",
    "    def custom_shard_func(self, element):\n",
    "        return np.int64(0)\n",
    "\n",
    "    ds_train.save(path=bucket + \"/ds_train\", shard_func=custom_shard_func)\n",
    "    ds_test.save(path=bucket + \"/ds_test\", shard_func=custom_shard_func)\n",
    "\n",
    "    return bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4e525ff9-98f1-42d8-b21e-1de7e960a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "47f2dc65-40af-4304-87d0-a789dc816b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['tensorflow==2.11.0', 'keras'],\n",
    "    base_image='gcr.io/deeplearning-platform-release/tf-gpu.2-11',\n",
    ")\n",
    "def create_model(text: str) -> str:\n",
    "    import tensorflow as tf\n",
    "    from keras import models, layers, datasets, applications\n",
    "\n",
    "    #bucket = 'gs://tfds-dir3'\n",
    "    #bucket = 'gs://pipeline-tester3/keras_model'\n",
    "    bucket = 'gs://workbench-ron-tensor'\n",
    "\n",
    "    # check for GPU:\n",
    "    print('\\n\\n GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print('\\n\\n')\n",
    "\n",
    "    #Multi GPU strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        model.add(layers.Dense(10))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print('\\n\\n' + str(model.summary()) + '\\n\\n')\n",
    "    model.save(bucket + \"/untrained-model\")\n",
    "    return \"model saved:\" + bucket\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "17596be6-7735-4c6d-a2ba-cc4365a285fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "86b04618-33d5-495d-a60b-091ec29d1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['tensorflow==2.11.0', 'tensorflow-datasets'],\n",
    "    base_image='gcr.io/deeplearning-platform-release/tf-gpu.2-11',\n",
    ")\n",
    "def train_model(text: str) -> str:\n",
    "    import tensorflow_datasets as tfds\n",
    "    import tensorflow as tf\n",
    "\n",
    "\n",
    "    # check for GPU:\n",
    "    print('\\n\\n GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print('\\n\\n')\n",
    "\n",
    "    #Storage buckets\n",
    "    # bucket = 'gs://tfds-dir3'\n",
    "    # bucket = 'gs://pipeline-tester3/keras_model'\n",
    "    bucket = 'gs://workbench-ron-tensor'\n",
    "\n",
    "    #Multi GPU strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "   \n",
    "\n",
    "    # load data from gcs bucket\n",
    "    ds_train = tf.data.Dataset.load(bucket + \"/ds_train\")\n",
    "    ds_test = tf.data.Dataset.load(bucket + \"/ds_test\")\n",
    "\n",
    "    print(\"\\n\\n finish loading training data\")\n",
    "    print(\"\\n\\n finish loading test data \\n\\n\")\n",
    "\n",
    "    # load model from gcs\n",
    "    model = tf.keras.models.load_model(bucket + '/untrained-model')\n",
    "\n",
    "    # Create and Train the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=30,\n",
    "    validation_data=ds_test,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n history\" + str(history))\n",
    "\n",
    "    #Save the model\n",
    "    model.save(bucket + \"/keras_ model\")\n",
    "\n",
    "    return \"model trained\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d6c6d29f-9270-48c6-aa91-7e1320df09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8112a9a8-9679-4655-974d-36d5cfaeb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING THE TRAIN_MODEL COMPONENT TWICE WITH **TWO DIFFERENT GPU CONFIGURATIONS** TO SHOW \n",
    "# THE DIFFERENCE IN TRAINING TIME THAT HARDWARE ACCELERATORS CAN GIVE\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='simple-pipeline',\n",
    "    description='testing pipeline',\n",
    "    pipeline_root=pipeline_root_path\n",
    ")\n",
    "def workbench_cnn_pipeline(    \n",
    "    bucket: str = 'gs://workbench-ron-tensor',\n",
    "    project: str = 'tensor-1-1',\n",
    "):\n",
    "    ingestion_task = ingest_data()\n",
    "    create_model_task = create_model(text=ingestion_task.output).set_accelerator_type('NVIDIA_TESLA_V100').set_cpu_limit('4').set_memory_limit('16G').set_accelerator_limit(4)\n",
    "    train_model_task_4_V100_GPUs = train_model(text=create_model_task.output)\\\n",
    "        .set_accelerator_type('NVIDIA_TESLA_V100')\\\n",
    "        .set_cpu_limit('4')\\\n",
    "        .set_memory_limit('16G')\\\n",
    "        .set_accelerator_limit(4)\\\n",
    "        .set_display_name('4 x V100 GPUS ')\n",
    "    train_model_task_2_V100_GPUs = train_model(text=create_model_task.output)\\\n",
    "        .set_accelerator_type('NVIDIA_TESLA_V100')\\\n",
    "        .set_cpu_limit('4')\\\n",
    "        .set_memory_limit('16G')\\\n",
    "        .set_accelerator_limit(2)\\\n",
    "        .set_display_name('2 x V100 GPUS ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "be44302d-6d65-457a-9fb8-fab041170e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE THE PIPELINE TO CREATE THE PIPELINE JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "12caac78-661b-496f-bef2-78c7143a4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    compiler.Compiler().compile(\n",
    "        pipeline_func=workbench_cnn_pipeline,\n",
    "        package_path='/home/jupyter/tensor project/tensorflow/workbench_demo/workbench_cnn_pipeline.json'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ce4cff8e-7434-4ee8-b48e-f7c32dfff745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _Config.init of <google.cloud.aiplatform.initializer._Config object at 0x7fde091b5710>>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8fc4352e-1c0d-4d2b-b67b-0d9eae980f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/10566138111/locations/us-central1/pipelineJobs/simple-pipeline-20230424181553\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/10566138111/locations/us-central1/pipelineJobs/simple-pipeline-20230424181553')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/simple-pipeline-20230424181553?project=10566138111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job = aiplatform.PipelineJob(display_name = 'workbench_cnn_pipeline',\n",
    "                             template_path = '/home/jupyter/tensor project/tensorflow/workbench_demo/workbench_cnn_pipeline.json',\n",
    "                             enable_caching=True,\n",
    "                             )\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "09197e1c-3afc-4526-afec-e25ccd1ab841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MODEL TO MODEL REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0b32a932-dfb0-4388-b237-2c0d8ab0dca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/10566138111/locations/us-central1/models/5538945955572744192/operations/8289288324389011456\n",
      "Model created. Resource name: projects/10566138111/locations/us-central1/models/5538945955572744192@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/10566138111/locations/us-central1/models/5538945955572744192@1')\n"
     ]
    }
   ],
   "source": [
    "#Storage buckets\n",
    "# bucket = 'gs://tfds-dir3'\n",
    "# bucket = 'gs://pipeline-tester3'\n",
    "bucket = 'gs://workbench-ron-tensor'\n",
    "model_uri = bucket + \"/keras_ model\"\n",
    "    \n",
    "my_model = aiplatform.Model.upload(display_name='keras-model', \n",
    "                                   artifact_uri=model_uri, \n",
    "                                   serving_container_image_uri= 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest')\n",
    "\n",
    "#Output models ID and modely registry uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82cc41-dd52-4426-8ca6-3891fc0f3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPLOY MODEL TO ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "25a7d804-9b89-42d6-980f-09a73ac3b148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/10566138111/locations/us-central1/endpoints/2186968210065063936/operations/2861324873500721152\n",
      "Endpoint created. Resource name: projects/10566138111/locations/us-central1/endpoints/2186968210065063936\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/10566138111/locations/us-central1/endpoints/2186968210065063936')\n",
      "Deploying model to Endpoint : projects/10566138111/locations/us-central1/endpoints/2186968210065063936\n",
      "Deploy Endpoint model backing LRO: projects/10566138111/locations/us-central1/endpoints/2186968210065063936/operations/8135040037151571968\n",
      "Endpoint model deployed. Resource name: projects/10566138111/locations/us-central1/endpoints/2186968210065063936\n"
     ]
    }
   ],
   "source": [
    "# This aiplatform.Model(\"projects/{PROJECT_NUMBER}/locations/us-central1/models/{MODEL_ID}\") \n",
    "#is the output from the above cmd\n",
    "\n",
    "my_model = aiplatform.Model('projects/10566138111/locations/us-central1/models/5538945955572744192@1')\n",
    "\n",
    "\n",
    "endpoint = my_model.deploy(\n",
    "     deployed_model_display_name='keras-endpoint',\n",
    "     traffic_split={\"0\": 100},\n",
    "     machine_type=\"n1-standard-4\",\n",
    "     accelerator_count=0,\n",
    "     min_replica_count=1,\n",
    "     max_replica_count=1,\n",
    "   )\n",
    "\n",
    "#Outputs Endpoint name to use for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cf548-c392-49b5-81ed-13c644bcd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET PREDICTIONS FROM DEPLOYED LOCAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a2009be8-58cf-48b5-bfdb-3cda7379ca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n",
      "The local model predicts this image most likely belongs to airplane with a 100.00 percent confidence.\n",
      "gs://workbench-ron-tensor\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import base64\n",
    "from typing import List, Dict\n",
    "import argparse\n",
    "import io\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "path_image = \"/home/jupyter/tensor project/tensorflow/workbench_demo/test_images/image_0_6.jpg\"\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    path_image, target_size=(32, 32))\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "model = tf.keras.models.load_model(bucket + \"/keras_ model\")\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(\n",
    "    \"The local model predicts this image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n",
    "\n",
    "print(bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3f4a1-c33e-404c-94d1-617b46af498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET PREDICTIONS FROM MODEL DEPLOYED AT ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4fefcc5d-e87a-4edf-9974-238545b43ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "The Vertex AI Endpoint predicts this image most likely belongs to airplane with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import base64\n",
    "from typing import List, Dict\n",
    "import argparse\n",
    "import io\n",
    "import tensorflow as tf\n",
    "\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    endpoint_name='projects/10566138111/locations/us-central1/endpoints/2186968210065063936')\n",
    "\n",
    "path_image = \"/home/jupyter/tensor project/tensorflow/workbench_demo/test_images/image_0_3.jpg\"\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "\n",
    "image_data = [np.asarray(Image.open(path_image))]\n",
    "test_image = [(image_data[0] / 255.0).astype(np.float32).tolist()]\n",
    "\n",
    "endpoint_prediction = endpoint.predict(instances=test_image)\n",
    "endpoint_score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\"\\n\" + \"\\n\")\n",
    "print(\n",
    "    \"The Vertex AI Endpoint predicts this image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(endpoint_score)], 100 * np.max(endpoint_score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac354c6-e586-4a9d-bfdc-fdf720352c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
