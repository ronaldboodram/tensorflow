{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6408d75-e80a-49dd-ab25-7462584e854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics, Dataset\n",
    "from typing import NamedTuple\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ca5698-7fc6-4121-aea8-cf9e136b812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'qwiklabs-gcp-03-6e0d35a97dd4'\n",
    "pipeline_root_path = 'gs://pipeline-tester3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab4a2a6-990f-4dcb-a0f2-cedad0131e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INGEST THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb78d84c-2a34-436d-9a4b-5d1cf956fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['tensorflow==2.11.0', 'tensorflow-datasets', 'numpy==1.21.6']\n",
    ")\n",
    "def ingest_data() -> str:\n",
    "    import tensorflow_datasets as tfds\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "\n",
    "    print(\"\\n\\n\" + \"Tensorflow version is: \" + tf.__version__ + \"\\n\\n\")\n",
    "    print(\"\\n\\n\" + \"Tfds version is: \" + tfds.__version__ + \"\\n\\n\")\n",
    "\n",
    "    validation_split = 10\n",
    "    #bucket = 'gs://tfds-dir3'\n",
    "    #bucket = 'gs://pipeline-tester3'\n",
    "    bucket = 'gs://workbench-ron-tensor'\n",
    "\n",
    "    # test_ds, cifar10_info = tfds.load('cifar10', split='test', with_info=True, as_supervised=True, shuffle_files=True, data_dir=\"gs://tfds-dir\")\n",
    "    # test_ds = tfds.load('cifar10', split='test', as_supervised=True, shuffle_files=True, data_dir=bucket + \"/test\", try_gcs=True)\n",
    "    # validation_ds = tfds.load('cifar10', split=f'train[:{validation_split}%]', as_supervised=True, data_dir=bucket + \"/valid\", try_gcs=True)\n",
    "    # training_ds = tfds.load('cifar10', split=f'train[{validation_split}%:]', as_supervised=True, data_dir=bucket + \"/train\", try_gcs=True)\n",
    "\n",
    "    test_ds = tfds.load('cifar10', split='test', as_supervised=True, shuffle_files=True)\n",
    "    validation_ds = tfds.load('cifar10', split=f'train[:{validation_split}%]', as_supervised=True)\n",
    "    training_ds = tfds.load('cifar10', split=f'train[{validation_split}%:]', as_supervised=True)\n",
    "\n",
    "    # need the \"self\" parameter as their is an implicit argument in the custom_shard_func\n",
    "    # that gives an error saying one arg expected but two were given\n",
    "    def custom_shard_func(self, element):\n",
    "        return np.int64(0)\n",
    "\n",
    "    training_ds.save(\n",
    "        path=bucket + \"/train_ds\", shard_func=custom_shard_func)\n",
    "\n",
    "    validation_ds.save(\n",
    "        path=bucket + \"/valid_ds\", shard_func=custom_shard_func)\n",
    "\n",
    "    test_ds.save(\n",
    "        path=bucket + \"/test_ds\", shard_func=custom_shard_func)\n",
    "\n",
    "    return bucket\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e525ff9-98f1-42d8-b21e-1de7e960a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f2dc65-40af-4304-87d0-a789dc816b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['tensorflow==2.11.0', 'keras'],\n",
    "    base_image='gcr.io/deeplearning-platform-release/tf-gpu.2-11',\n",
    ")\n",
    "def create_model(text: str) -> str:\n",
    "    import tensorflow as tf\n",
    "    from keras import applications\n",
    "\n",
    "    #bucket = 'gs://tfds-dir3'\n",
    "    # bucket = 'gs://pipeline-tester3'\n",
    "    bucket = 'gs://workbench-ron-tensor'\n",
    "\n",
    "    # check for GPU:\n",
    "    print('\\n\\n GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print('\\n\\n')\n",
    "\n",
    "    #Multi GPU strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        new_model = tf.keras.Sequential([\n",
    "            # applications.ResNet50(weights=None, include_top=False, input_shape=(32, 32, 3)),\n",
    "            tf.keras.layers.InputLayer((32, 32, 3)),\n",
    "            tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "            tf.keras.layers.MaxPool2D(),\n",
    "            tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "            tf.keras.layers.MaxPool2D(),\n",
    "            tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "            tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "            tf.keras.layers.MaxPool2D(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print('\\n\\n' + str(new_model.summary()) + '\\n\\n')\n",
    "    new_model.save(bucket + \"/untrained-model\")\n",
    "    return \"model saved:\" + bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17596be6-7735-4c6d-a2ba-cc4365a285fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b04618-33d5-495d-a60b-091ec29d1ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['tensorflow==2.11.0', 'tensorflow-datasets'],\n",
    "    output_component_file=\"train_model.yaml\",\n",
    "    base_image='gcr.io/deeplearning-platform-release/tf-gpu.2-11',\n",
    ")\n",
    "def train_model(text: str) -> str:\n",
    "    import tensorflow_datasets as tfds\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    import datetime\n",
    "\n",
    "    # check for GPU:\n",
    "    print('\\n\\n GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print('\\n\\n')\n",
    "\n",
    "    # env variable that tells tensorboard where to store logs\n",
    "    #os.environ['AIP_TENSORBOARD_LOG_DIR']\n",
    "    #os.environ['gs://tensorboard3']\n",
    "\n",
    "    #Storage buckets\n",
    "    # bucket = 'gs://tfds-dir3'\n",
    "    # bucket = 'gs://pipeline-tester3'\n",
    "    bucket = 'gs://workbench-ron-tensor'\n",
    "    \n",
    "    #Multi GPU strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    # batch size\n",
    "    batch_size = 32 * strategy.num_replicas_in_sync\n",
    "\n",
    "    # load data from gcs bucket\n",
    "    model_name = 'simple model'\n",
    "    train_data = tf.data.Dataset.load(bucket + \"/train_ds\")\n",
    "    train_data = train_data.map(lambda f, l: (tf.cast(f, tf.float64) / 255, l))\n",
    "    train_data = train_data.shuffle(buffer_size=5000)\n",
    "    train_data = train_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    print(\"\\n\\n finish loading training data \\n\\n\")\n",
    "\n",
    "    valid_data = tf.data.Dataset.load(bucket + \"/valid_ds\")\n",
    "    valid_data = valid_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    print(\"\\n\\n finish loading validation data \\n\\n\")\n",
    "    test_data = tf.data.Dataset.load(bucket + \"/test_ds\")\n",
    "    test_data = test_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    print(\"\\n\\n finish loading test data \\n\\n\")\n",
    "\n",
    "    # load model from gcs\n",
    "    model = tf.keras.models.load_model(bucket + '/untrained-model')\n",
    "\n",
    "    # Create training callbacks\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping('val_loss', patience=5, restore_best_weights=True)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=bucket + f'/ckpts/cifar10-{model_name}-' + '{epoch:02d}-{val_accuracy:.4f}')\n",
    "    log_dir = bucket + \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_data, validation_data=valid_data, epochs=30, callbacks=[earlystop, checkpoint])\n",
    "    #history = model.fit(train_data, validation_data=valid_data, epochs=10)\n",
    "    print('\\n\\n history\\n' + str(history) + '\\n\\n')\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(test_data)\n",
    "    print('\\n\\n'  + f'Test accuracy: {test_acc * 100:.2f}%' + '\\n\\n')\n",
    "\n",
    "    #Save the model\n",
    "    model.save(bucket + \"/simple_ model\")\n",
    "\n",
    "    return \"model trained\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c6d29f-9270-48c6-aa91-7e1320df09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8112a9a8-9679-4655-974d-36d5cfaeb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='simple-pipeline',\n",
    "    description='testing pipeline',\n",
    "    pipeline_root=pipeline_root_path\n",
    ")\n",
    "def workbench_cnn_pipeline(    \n",
    "    bucket: str = 'gs://workbench-ron-tensor',\n",
    "    project: str = 'tensor-1-1',\n",
    "):\n",
    "    ingestion_task = ingest_data()\n",
    "    create_model_task = create_model(text=ingestion_task.output).set_accelerator_type('NVIDIA_TESLA_V100').set_cpu_limit('4').set_memory_limit('16G').set_accelerator_limit(4)\n",
    "    train_model_task_4_V100_GPUs = train_model(text=create_model_task.output)\\\n",
    "        .set_accelerator_type('NVIDIA_TESLA_V100')\\\n",
    "        .set_cpu_limit('2')\\\n",
    "        .set_memory_limit('16G')\\\n",
    "        .set_accelerator_limit(4)\\\n",
    "        .set_display_name('2 x V100 GPUS ')\n",
    "    train_model_task_2_V100_GPUs = train_model(text=create_model_task.output)\\\n",
    "        .set_accelerator_type('NVIDIA_TESLA_V100')\\\n",
    "        .set_cpu_limit('1')\\\n",
    "        .set_memory_limit('16G')\\\n",
    "        .set_accelerator_limit(2)\\\n",
    "        .set_display_name('1 x V100 GPUS ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be44302d-6d65-457a-9fb8-fab041170e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE THE PIPELINE TO CREATE THE PIPELINE JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12caac78-661b-496f-bef2-78c7143a4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    compiler.Compiler().compile(\n",
    "        pipeline_func=workbench_cnn_pipeline,\n",
    "        package_path='workbench_cnn_pipeline.json'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce4cff8e-7434-4ee8-b48e-f7c32dfff745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _Config.init of <google.cloud.aiplatform.initializer._Config object at 0x7f31fee4ae50>>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc4352e-1c0d-4d2b-b67b-0d9eae980f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/10566138111/locations/us-central1/pipelineJobs/simple-pipeline-20230423172814\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/10566138111/locations/us-central1/pipelineJobs/simple-pipeline-20230423172814')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/simple-pipeline-20230423172814?project=10566138111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job = aiplatform.PipelineJob(display_name = 'workbench_cnn_pipeline',\n",
    "                             template_path = 'simple-model-pipeline.json',\n",
    "                             enable_caching=True,\n",
    "                             )\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b32a932-dfb0-4388-b237-2c0d8ab0dca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
