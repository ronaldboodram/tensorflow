{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6408d75-e80a-49dd-ab25-7462584e854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics, Dataset\n",
    "from typing import NamedTuple\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ca5698-7fc6-4121-aea8-cf9e136b812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'qwiklabs-gcp-03-6e0d35a97dd4'\n",
    "pipeline_root_path = 'gs://pipeline-tester3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab4a2a6-990f-4dcb-a0f2-cedad0131e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INGEST THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb78d84c-2a34-436d-9a4b-5d1cf956fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['tensorflow==2.11.0', 'tensorflow-datasets', 'numpy==1.21.6']\n",
    ")\n",
    "def ingest_data() -> str:\n",
    "    import tensorflow_datasets as tfds\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "\n",
    "    print(\"\\n\\n\" + \"Tensorflow version is: \" + tf.__version__ + \"\\n\\n\")\n",
    "    print(\"\\n\\n\" + \"Tfds version is: \" + tfds.__version__ + \"\\n\\n\")\n",
    "\n",
    "    validation_split = 10\n",
    "    #bucket = 'gs://tfds-dir3'\n",
    "    #bucket = 'gs://pipeline-tester3'\n",
    "    bucket = 'gs://workbench-ron-tensor'\n",
    "\n",
    "    # test_ds, cifar10_info = tfds.load('cifar10', split='test', with_info=True, as_supervised=True, shuffle_files=True, data_dir=\"gs://tfds-dir\")\n",
    "    # test_ds = tfds.load('cifar10', split='test', as_supervised=True, shuffle_files=True, data_dir=bucket + \"/test\", try_gcs=True)\n",
    "    # validation_ds = tfds.load('cifar10', split=f'train[:{validation_split}%]', as_supervised=True, data_dir=bucket + \"/valid\", try_gcs=True)\n",
    "    # training_ds = tfds.load('cifar10', split=f'train[{validation_split}%:]', as_supervised=True, data_dir=bucket + \"/train\", try_gcs=True)\n",
    "\n",
    "    test_ds = tfds.load('cifar10', split='test', as_supervised=True, shuffle_files=True)\n",
    "    validation_ds = tfds.load('cifar10', split=f'train[:{validation_split}%]', as_supervised=True)\n",
    "    training_ds = tfds.load('cifar10', split=f'train[{validation_split}%:]', as_supervised=True)\n",
    "\n",
    "    # need the \"self\" parameter as their is an implicit argument in the custom_shard_func\n",
    "    # that gives an error saying one arg expected but two were given\n",
    "    def custom_shard_func(self, element):\n",
    "        return np.int64(0)\n",
    "\n",
    "    training_ds.save(\n",
    "        path=bucket + \"/train_ds\", shard_func=custom_shard_func)\n",
    "\n",
    "    validation_ds.save(\n",
    "        path=bucket + \"/valid_ds\", shard_func=custom_shard_func)\n",
    "\n",
    "    test_ds.save(\n",
    "        path=bucket + \"/test_ds\", shard_func=custom_shard_func)\n",
    "\n",
    "    return bucket\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e525ff9-98f1-42d8-b21e-1de7e960a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f2dc65-40af-4304-87d0-a789dc816b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['tensorflow==2.11.0', 'keras'],\n",
    "    base_image='gcr.io/deeplearning-platform-release/tf-gpu.2-11',\n",
    ")\n",
    "def create_model(text: str) -> str:\n",
    "    import tensorflow as tf\n",
    "    from keras import applications\n",
    "\n",
    "    #bucket = 'gs://tfds-dir3'\n",
    "    # bucket = 'gs://pipeline-tester3'\n",
    "    bucket = 'gs://workbench-ron-tensor'\n",
    "\n",
    "    # check for GPU:\n",
    "    print('\\n\\n GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print('\\n\\n')\n",
    "\n",
    "    #Multi GPU strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        new_model = tf.keras.Sequential([\n",
    "            # applications.ResNet50(weights=None, include_top=False, input_shape=(32, 32, 3)),\n",
    "            tf.keras.layers.InputLayer((32, 32, 3)),\n",
    "            tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "            tf.keras.layers.MaxPool2D(),\n",
    "            tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "            tf.keras.layers.MaxPool2D(),\n",
    "            tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "            tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "            tf.keras.layers.MaxPool2D(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print('\\n\\n' + str(new_model.summary()) + '\\n\\n')\n",
    "    new_model.save(bucket + \"/untrained-model\")\n",
    "    return \"model saved:\" + bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17596be6-7735-4c6d-a2ba-cc4365a285fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b04618-33d5-495d-a60b-091ec29d1ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['tensorflow==2.11.0', 'tensorflow-datasets'],\n",
    "    output_component_file=\"train_model.yaml\",\n",
    "    base_image='gcr.io/deeplearning-platform-release/tf-gpu.2-11',\n",
    ")\n",
    "def train_model(text: str) -> str:\n",
    "    import tensorflow_datasets as tfds\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    import datetime\n",
    "\n",
    "    # check for GPU:\n",
    "    print('\\n\\n GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print('\\n\\n')\n",
    "\n",
    "    # env variable that tells tensorboard where to store logs\n",
    "    #os.environ['AIP_TENSORBOARD_LOG_DIR']\n",
    "    #os.environ['gs://tensorboard3']\n",
    "\n",
    "    #Storage buckets\n",
    "    # bucket = 'gs://tfds-dir3'\n",
    "    # bucket = 'gs://pipeline-tester3'\n",
    "    bucket = 'gs://workbench-ron-tensor'\n",
    "    \n",
    "    #Multi GPU strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    # batch size\n",
    "    batch_size = 32 * strategy.num_replicas_in_sync\n",
    "\n",
    "    # load data from gcs bucket\n",
    "    model_name = 'simple model'\n",
    "    train_data = tf.data.Dataset.load(bucket + \"/train_ds\")\n",
    "    train_data = train_data.map(lambda f, l: (tf.cast(f, tf.float64) / 255, l))\n",
    "    train_data = train_data.shuffle(buffer_size=5000)\n",
    "    train_data = train_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    print(\"\\n\\n finish loading training data \\n\\n\")\n",
    "\n",
    "    valid_data = tf.data.Dataset.load(bucket + \"/valid_ds\")\n",
    "    valid_data = valid_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    print(\"\\n\\n finish loading validation data \\n\\n\")\n",
    "    test_data = tf.data.Dataset.load(bucket + \"/test_ds\")\n",
    "    test_data = test_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    print(\"\\n\\n finish loading test data \\n\\n\")\n",
    "\n",
    "    # load model from gcs\n",
    "    model = tf.keras.models.load_model(bucket + '/untrained-model')\n",
    "\n",
    "    # Create training callbacks\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping('val_loss', patience=5, restore_best_weights=True)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=bucket + f'/ckpts/cifar10-{model_name}-' + '{epoch:02d}-{val_accuracy:.4f}')\n",
    "    log_dir = bucket + \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_data, validation_data=valid_data, epochs=30, callbacks=[earlystop, checkpoint])\n",
    "    #history = model.fit(train_data, validation_data=valid_data, epochs=10)\n",
    "    print('\\n\\n history\\n' + str(history) + '\\n\\n')\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(test_data)\n",
    "    print('\\n\\n'  + f'Test accuracy: {test_acc * 100:.2f}%' + '\\n\\n')\n",
    "\n",
    "    #Save the model\n",
    "    model.save(bucket + \"/simple_ model\")\n",
    "\n",
    "    return \"model trained\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c6d29f-9270-48c6-aa91-7e1320df09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8112a9a8-9679-4655-974d-36d5cfaeb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='simple-pipeline',\n",
    "    description='testing pipeline',\n",
    "    pipeline_root=pipeline_root_path\n",
    ")\n",
    "def workbench_cnn_pipeline(    \n",
    "    bucket: str = 'gs://workbench-ron-tensor',\n",
    "    project: str = 'tensor-1-1',\n",
    "):\n",
    "    ingestion_task = ingest_data()\n",
    "    create_model_task = create_model(text=ingestion_task.output).set_accelerator_type('NVIDIA_TESLA_V100').set_cpu_limit('4').set_memory_limit('16G').set_accelerator_limit(4)\n",
    "    train_model_task_4_V100_GPUs = train_model(text=create_model_task.output)\\\n",
    "        .set_accelerator_type('NVIDIA_TESLA_V100')\\\n",
    "        .set_cpu_limit('2')\\\n",
    "        .set_memory_limit('16G')\\\n",
    "        .set_accelerator_limit(4)\\\n",
    "        .set_display_name('2 x V100 GPUS ')\n",
    "    train_model_task_2_V100_GPUs = train_model(text=create_model_task.output)\\\n",
    "        .set_accelerator_type('NVIDIA_TESLA_V100')\\\n",
    "        .set_cpu_limit('1')\\\n",
    "        .set_memory_limit('16G')\\\n",
    "        .set_accelerator_limit(2)\\\n",
    "        .set_display_name('1 x V100 GPUS ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be44302d-6d65-457a-9fb8-fab041170e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE THE PIPELINE TO CREATE THE PIPELINE JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12caac78-661b-496f-bef2-78c7143a4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    compiler.Compiler().compile(\n",
    "        pipeline_func=workbench_cnn_pipeline,\n",
    "        package_path='workbench_cnn_pipeline.json'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce4cff8e-7434-4ee8-b48e-f7c32dfff745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _Config.init of <google.cloud.aiplatform.initializer._Config object at 0x7f31fee4ae50>>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc4352e-1c0d-4d2b-b67b-0d9eae980f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/10566138111/locations/us-central1/pipelineJobs/simple-pipeline-20230423172814\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/10566138111/locations/us-central1/pipelineJobs/simple-pipeline-20230423172814')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/simple-pipeline-20230423172814?project=10566138111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job = aiplatform.PipelineJob(display_name = 'workbench_cnn_pipeline',\n",
    "                             template_path = 'simple-model-pipeline.json',\n",
    "                             enable_caching=True,\n",
    "                             )\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09197e1c-3afc-4526-afec-e25ccd1ab841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MODEL TO MODEL REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b32a932-dfb0-4388-b237-2c0d8ab0dca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/10566138111/locations/us-central1/models/9169691680163495936/operations/8430940606418649088\n",
      "Model created. Resource name: projects/10566138111/locations/us-central1/models/9169691680163495936@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/10566138111/locations/us-central1/models/9169691680163495936@1')\n"
     ]
    }
   ],
   "source": [
    "#Storage buckets\n",
    " # bucket = 'gs://tfds-dir3'\n",
    "bucket = 'gs://pipeline-tester3'\n",
    "# bucket = 'gs://workbench-ron-tensor'\n",
    "model_uri = bucket + \"/keras_model/keras_ model\"\n",
    "    \n",
    "my_model = aiplatform.Model.upload(display_name='keras-model', \n",
    "                                   artifact_uri=model_uri, \n",
    "                                   serving_container_image_uri= 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82cc41-dd52-4426-8ca6-3891fc0f3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPLOY MODEL TO ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25a7d804-9b89-42d6-980f-09a73ac3b148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/10566138111/locations/us-central1/endpoints/4942326757085675520/operations/4980761079387783168\n",
      "Endpoint created. Resource name: projects/10566138111/locations/us-central1/endpoints/4942326757085675520\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/10566138111/locations/us-central1/endpoints/4942326757085675520')\n",
      "Deploying model to Endpoint : projects/10566138111/locations/us-central1/endpoints/4942326757085675520\n",
      "Deploy Endpoint model backing LRO: projects/10566138111/locations/us-central1/endpoints/4942326757085675520/operations/4944732282368819200\n",
      "Endpoint model deployed. Resource name: projects/10566138111/locations/us-central1/endpoints/4942326757085675520\n"
     ]
    }
   ],
   "source": [
    "# my_model = aiplatform.Model(\"projects/{PROJECT_NUMBER}/locations/us-central1/models/{MODEL_ID}\") \n",
    "my_model = aiplatform.Model('projects/10566138111/locations/us-central1/models/9169691680163495936@1')\n",
    "\n",
    "\n",
    "endpoint = my_model.deploy(\n",
    "     deployed_model_display_name='keras-endpoint',\n",
    "     traffic_split={\"0\": 100},\n",
    "     machine_type=\"n1-standard-4\",\n",
    "     accelerator_count=0,\n",
    "     min_replica_count=1,\n",
    "     max_replica_count=1,\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cf548-c392-49b5-81ed-13c644bcd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET PREDICTIONS FROM DEPLOYED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a2009be8-58cf-48b5-bfdb-3cda7379ca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step\n",
      "This image most likely belongs to automobile with a 23.20 percent confidence.\n",
      "\n",
      "Prediction(predictions=[[1.24650228, 3.32507133, -3.51667333, -2.77927899, -3.1631844, -4.86116123, -3.0711298, -0.0200937428, -1.09384143, 4.49908733]], deployed_model_id='4629350772239237120', model_version_id='1', model_resource_name='projects/10566138111/locations/us-central1/models/9169691680163495936', explanations=None)\n",
      "\n",
      "The Vertex AI Endpoint predicts this image most likely belongs to automobile with a 23.20 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import base64\n",
    "from typing import List, Dict\n",
    "import argparse\n",
    "import io\n",
    "import tensorflow as tf\n",
    "\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    endpoint_name='projects/10566138111/locations/us-central1/endpoints/4942326757085675520')\n",
    "\n",
    "path_image = \"/home/jupyter/tensor project/tensorflow/workbench_demo/car.jpeg\"\n",
    "img = tf.keras.utils.load_img(\n",
    "    path_image, target_size=(32, 32))\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "model = tf.keras.models.load_model(bucket + \"/simple_ model\")\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n",
    "\n",
    "\n",
    "image_data = [np.asarray(Image.open(path_image))]\n",
    "test_image = [(image_data[0] / 255.0).astype(np.float32).tolist()]\n",
    "\n",
    "\n",
    "endpoint_prediction = endpoint.predict(instances=test_image)\n",
    "endpoint_score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\"\\n\" + str(endpoint_prediction) + \"\\n\")\n",
    "print(\n",
    "    \"The Vertex AI Endpoint predicts this image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(endpoint_score)], 100 * np.max(endpoint_score))\n",
    ")\n",
    "\n",
    "#endpoint.predict(instances=img_array).predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# image_open = open(path_image, 'rb')\n",
    "# read_image = image_open.read()\n",
    "# image_decode = tf.image.decode_jpeg(read_image)\n",
    "# print(\"This is the Shape of resized image\",image_decode.shape)\n",
    "# resize_image = tf.image.resize(image_decode, [32, 32])\n",
    "# print(\"This is the Shape of resized image\",resize_image.shape)\n",
    "\n",
    "\n",
    "# # im = Image.open(IMAGE_PATH)\n",
    "\n",
    "# x_test = np.asarray(resize_image).astype(np.float32).tolist()\n",
    "\n",
    "# byte_array = base64.b64decode(path_image)\n",
    "# image = Image.open(io.BytesIO(byte_array))\n",
    "# image = np.asarray(image + \"=\" * 4).astype(np.float32).tolist()\n",
    "\n",
    "# # image = image / 255.0\n",
    "# # image = image.reshape(28, 28, 1)\n",
    "\n",
    "\n",
    "# endpoint.predict(instances=resize_image).predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983579c5-6799-47ed-9871-7bc14b55f3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab195115-198a-4465-b3ba-dcb36e015d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
